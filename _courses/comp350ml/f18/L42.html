<!DOCTYPE html>
<html>
  <head>
    <title>Topic Models</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      body {
        font-family: 'Muli';
        font-size: 140%;
      }
      h1, h2 {
        font-family: 'Garamond';
        font-weight: normal;
        margin-top: 10px;
        margin-bottom: 10px;
      }
      .remark-slide-content h1 {
        font-size: 70px;
        text-align: center;
      }
      .remark-slide-content p, .remark-slide-content li {
        font-size:30px;
        line-height: 1.4;
      }
      .remark-code {
        font-size:30px;
      }
      .remark-slide-content p {
          margin: 5px;
      }
      .remark-slide-container .spacious p,
      .remark-slide-container .spacious li{
          margin-bottom: 50px;
          margin-top: 50px;
      }
      .remark-slide-container .spacious h1{
          margin-bottom: 50px;
      }
      .remark-slide-container .some-space p,
      .remark-slide-container .some-space li,
      .remark-slide-container .some-space h1{
          margin-bottom: 30px;
      }
      .reset-column {
          overflow: auto;
          width: 100%;
      }
      .remark-slide-content .compact p, .remark-slide-content .compact li, .remark-slide-content .compact pre, .remark-slide-content .compact .MathJax_Display{
          font-size: 30px;
          line-height: 1.1;
          display: block;
          margin: 2px 0;
      }
      .padding-top {
          padding-top: 100px;
      }
      .remark-slide-content .smaller p, .remark-slide-content .smaller li,
      .remark-slide-content .smaller .remark-code, .remark-slide-content .smaller a{
          font-size: 25px;
      }
      .normal {
          font-size: 30px;
      }
      .remark-slide-content .smallest p, .remark-slide-content .smallest li,
      .remark-slide-content .smallest .remark-code, .remark-slide-content .smallest a{
          font-size: 15px;
      }

      .quote_author {
          display: block;
          text-align: right;
          margin-top: 20px;
          font-size: 30px;
          font-family: 'Garamond';
      }
      .larger, .larger .remark-code {
          font-size: 40px;
      }
      .largest, .largest .remark-code {
          font-size: 50px;
      }
      .left-column, .right-column {
          width: 48%;
      }
      .right-column{
          float: right;
      }
      .left-column{
          float: left;
      }
      .narrow-right-column {
          float: right;
          width: 32%
      }
      .wide-left-column {
          float: left;
          width: 65%
      }
      .invisible {
          visibility: hidden
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .column:first-of-type {float:left}
      .column:last-of-type {float:right}
      .tiny-code .remark-code, .remark-inline-code .tiny-code{
        font-size: 15px;
      }
      .split-40 .column:first-of-type {width: 50%}
      .split-40 .column:last-of-type {width: 50%}
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# LSA & Topic Models

Based on slides by Andreas Müller

???

Theme: Extracting info from text in an unsupervised way

---

# Beyond Bags of Words

Limitations of bag of words:
- Semantics of words not captured
- Synonymous words not represented
- Very distributed representation of documents

???
- Want a more semantic representation of words, not just counts

---

# Latent Semantic Analysis (LSA)

- Reduce dimensionality of data.
- Can't use PCA: can't subtract the mean (sparse data)
- Instead of PCA: Just do SVD, truncate.
- "Semantic" features, dense representation.
- Easy to compute - convex optimization

???
- The way PCA is formulated is actually centers everything around the origin, i.e., it subtracts the mean
- Q: Why might this be bad for sparse data? A: no longer sparse!
- Instead we'll do a different matrix factorization called singular-value decomposition
- It's a very similar computation as PCA though

---

# LSA with Truncated SVD

.smaller[
```python
from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer(stop_words="english", min_df=4)
X_train = vect.fit_transform(text_train)
X_train.shape
```
```
(25000, 30462)
```
```python
from sklearn.decomposition import TruncatedSVD
lsa = TruncatedSVD(n_components=100)
X_lsa = lsa.fit_transform(X_train)
lsa.components_.shape
```
```
(100, 30462)
```
]

???
 
- In the context of text this is LSA
- But in general it's SVD, so that's why sklearn called it that
- Q: What will X_lsa.shape print? A: (25000, 100)

---
.smaller[
```python
plt.semilogy(lsa.explained_variance_ratio_)
```
]

.center[
![:scale 70%](images/lsa_truncated_svd_plot.png)
]

???
- Similar to PCA before

---
# First Six Components

.center[
![:scale 100%](images/lsa_six_eigvec.png)
]

???
- 10 largest (longest) components
- These are actually eigenvectors for those who've had linear algebra
- First points in direction of mean
- You might hope to find something that distinguishes sentiments,
  though that's not clear here.

---

# Scale before LSA

```python
from sklearn.preprocessing import MaxAbsScaler
scaler = MaxAbsScaler()
X_scaled = scaler.fit_transform(X_train)

lsa_scaled = TruncatedSVD(n_components=100)
X_lsa_scaled = lsa_scaled.fit_transform(X_nscaled)
```

???
- "Movie" and "Film" were dominating first couple of components. Try to get rid of that effect.
- Equivalent of standard scaler except without subtracting the mean
- Movie and film still important, but not that dominant any more.
- Can also normalize so that length of the doc shouldn't matter
- e.g. doubling a document shouldn't affect anything, but if you don't
  normalize it changes the word counts!

---

# Components after scaling

.center[
![:scale 100%](images/lsa_six_eigvec_scaled.png)
]

???
- First still points in direction of mean
- Third one basically seems like "writer sophistication" (maybe pretentious)
- Four seems to correlate to bad reviews, I would think
- Actually 2 as well, so try plotting...

---
# Some Components Capture Sentiment

.smallest[
```python
plt.scatter(X_lsa_scaled[:, 1], X_lsa_scaled[:, 3], alpha=.1, c=y_train)
```
]

.center[
![:scale 35%](images/capture_sentiment.png)
]

.smallest[

```python
lr_lsa = LogisticRegression(C=100).fit(X_lsa_scaled[:, :10], y_train)
lr_lsa.score(X_test_lsa_scaled[:, :10], y_test)```
```
0.827
```
```python
lr_lsa.score(X_lsa_scaled[:, :10], y_train)
```
```
0.827
```
]

???
- Not competitive but reasonable for just 10 components (88 for full BOW)

---
class: center, middle

# Topic Models

---
class: spacious

# Motivation

- Each document is created as a mixture of topics
- Topics are distributions over words
- Learn topics and composition of documents simultaneously
- Unsupervised (and possibly ill-defined)

???
- Documents have a weighted vector of topic importances
- Topics have a distribution of words
- Draw words for a document based on weighted topic importances
- Will choose # topics: more topics = they are more specific, fewer = broader

---

# NMF for topic models

.center[
![:scale 80%](images/nmf.png)
]

???
- X is word counts now

---
# NMF objectives

`$$\large d_{Fro} = \frac{1}{2} ||X - HW||_{\mathrm{Fro}}^2 = \frac{1}{2} \sum_{i,j} (X_{ij} - (HW)_{ij})^2$$`

`$$\large d_{KL}(X, HW) = \sum_{i,j} \left(X_{ij} \log\left(\frac{X_{ij}}{(HW)_{ij}}\right) - X_{ij} + (HW)_{ij}\right)$$`

???
- Didn't go into detail last time
- Idea is just two ways to measure how much error you have, and the
algorithms find the H and W that minimize it
- Second one is original and has some mathematical properties that
  make it easier to optimize
- But first one is used more often today, default in sklearn

---

# NMF for topic models

.center[
![:scale 95%](images/nmf_1.png)
]
???
- Q: rows of H are what? A: documents
- Q: columns of H? A: topic importances for documents
- Q: rows of W? A: word distributions for topics, e.g., murder, scary for horror

---
# NMF on Scaled Data

.smallest[
```python
nmf_scale = NMF(n_components=100, verbose=10, tol=0.01)
nmf_scale.fit(X_scaled)
```
]

.center[
![:scale 65%](images/nmf_scaled_plot.png)
]

???
- Btw, runs much longer than LSA
- Very peaked, some are semantic
- Q: What do you think is interesting?
- 6 seems about musicals
- 4 maybe about TV shows
- 9 seems to talk about the processor movie making
- Q: What am I plotting here: rows/cols of H/W? A: rows of W!
- Only looking at those rows that have large absolute values
- i.e., sorted by "most active" components

---

class: center
# NMF components without scaling

![:scale 70%](images/nmf_topics_no_scaling.png)

???
- Without scaling
- Q: Anything interesting?
- 2 looks like about comics/superheros
- 7 seems like scifi (maybe trek/wars)


---
class: center

# NMF with tfidf

![:scale 70%](images/nmf_topics_tfidf.png)

???
- Similar thing but with tfidf

---


# NMF with tfidf and 10 components

.center[
![:scale 70%](images/tfidf_10comp_plot.png)
]

???
- Previously I got 100 NMF components and only showed some of them
- Now I'm just getting 10 components
- Q: How will the topics differ? (At a very high level)
- A: they will be more broad!
- 8 seems to mostly indicate bad, 7 horror?, 9 comedy?


---
class: center, middle

# Latent Dirichlet Allocation
# (the other LDA)

---

# The LDA Model

.center[
![:scale 90%](images/lda_model.png)
]

???

- Idea: Write down generative process for how each document is created
- Generative probabilistic model (similar to mixture model)
- Given some topics with word importances (e.g. genetics: gene, dna, etc.)
- Draw some topics with weights from some topic distribution
- Then for each word, decide on a topic, then draw a word from that topic's word distribution
- Only looks at word counts usually, not context
- Now: given a corpus of documents, can I figure out the distributions
  that were likely to lead to this?
- Learning is probabilistic inference
- Non-convex optimization (even harder than mixture models)

---

.center[
![:scale 70%](images/lda_params.png)
]

.smallest[
- For each topic $k$, draw $\phi_k \sim \text{Dirichlet}(\beta), k = 1 ... K$
- For each document $d$ draw $\theta_d \sim \text{Dirichlet}(\alpha), d = 1 ... D$
- For each word $i$ in document $d$:
	* Draw a topic index $z_{di} \sim \text{Multinomial}(\theta_d)$
	* Draw the observed word $w_{ij} \sim \text{Multinomial}(\beta_z)$


- (taken from Yang Ruan, Changsi An (http://salsahpc.indiana.edu/b649proj/proj3.html)
]
???
- Each circle is variable, rectangles are "plates", meaning a variable is duplicated
- K topics = multinomial distributions over words (how important is each word in a topic)
- For each document D, have a distribution
- "mixture weights" for each document:
	* How important is each topic for this document
    * Each document contains multiple topics!
- Only w is observed (word counts)
- alpha and beta are hyperparameters
- From the Dirichlet distribution, mostly so that the math is easier

---

# Two Schools (of solvers)

.left-column[
Gibbs Sampling
- Implements MCMC
- Standard procedure for any probabilistic model
- Very accuracte
- Very slow

]

.right-column[

Variational Inference
- Extension of expectation-maximization algorithm
- Deterministic
- fast(er)
- Less accurate solutions

]

???
- For solving, i.e., finding those distributions
- MCMC = Markov chain Monte Carlo
- Sampling is...well, sampling, so it's a random process 
- Variational kind of same idea as k-means; assignments and
  maximization

---
# Pick a solver

- “Small data” (<= 10k? Documents):
	* Gibbs sampling (lda package)
- “Medium data” (<= 1M? Documents):
	* Variational Inference (scikit-learn future default)
- “Large Data” (>1M? Documents):
	* Stochastic Variational Inference (scikit-learn current default)
	* SVI allows online learning (partial_fit)

- Edward by Dustin Tran: http://edwardlib.org/
	* Tensor-flow based framework for stochastic variational inference. 

???
- Remember SGD Lecture (and Leon Bottou):
	* More data beats better inference (often)
	* Stochastic is approximation to an approximation...
- Variational might be more sensitive to hyper parameters? Or give different results?
- If large data, choice: Tradeoff between exactness on small amount of
  data or approximating on large amount of data

---

.smallest[
```python
from sklearn.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components=10, learning_method="batch")
X_lda = lda.fit_transform(X_train)
```
]

.center[
![:scale 70%](images/lda_plot_1.png)
]
???
- Again, components not ordered. Changing # components changes
  everything.
- Very generic, similar to NMF(n_components=10).  TV Series, “family
  drama”, and “history / war” topics
- Only 10 components, so pretty generic

---


.smallest[
```python
lda100 = LatentDirichletAllocation(n_components=100, learning_method="batch")
X_lda100 = lda100.fit_transform(X_train)
```
]

.center[
![:scale 60%](images/lda_plot_2.png)
]

???
- 9 out of a 100, so probably more specific

---
.center[
![:scale 80%](images/lda_topics.png)
]

???
- Picked out some topics
- Viewing most important words
- Actually sometimes easier to see what's going on
- Note this is just picking topics, which *could* be useful for
  interpretation, but is not necessarily telling you anyting about the
  answers, which here is review sentiment.

---

# Hyper-Parameters

- $\alpha$ (or $\theta$) = doc_topic_prior
- $\beta$ (or $\eta$) = topic_word_prior
- Both dirichlet distributions
- Large value $\xrightarrow[]{}$ more dispersed
- Small value $\xrightarrow[]{}$ more concentrated

.center[
![:scale 50%](images/lda_params.png)
]

???
- Basically need somewhere to start

---

# Dirichlet Distributions

.left-column[
PDF: 
$\qquad \frac{1}{B(\alpha)} \Pi_{i=1}^{K}{x_i^{\alpha_i - 1}}$
]

.right-column[
Mean: 
$\qquad E[X_i] = \frac{\alpha_i}{\sum_k{\alpha_k}}$
]

.center[
![:scale 60%](images/dirichlet_dist.png)
]

???
- A distribution over distributions...
- Drawing one point; that's a multinomial distribution
- Each plot is a point you might draw from the distribution
- Each tells you three numbers, e.g. a distribution over 3 topics (must sum to 1)
- Top left: you don't know the relative importance of each topic,
  might be even, might be skewed (corners)
- Top right: each document will be about many topics (mix)
- Similar interpretation for drawing distributions over words

---

# Conjugate Prior

- Prior is called "conjugate" of the posterior has the same form as prior
$$ P(\theta | x) = \frac{p(x | \theta)p(\theta)}{\int_{}^{} p(x | \theta^\prime)p(\theta^\prime)d\theta^\prime} $$

--

Multinomial
$$ P(x_1, \dots, x_k |n, p_1, \dots, p_k) = \frac{n!}{x_1! \dots x_k!} p_1^{x_1} \dots p_k^{x^k}$$


Dirichlet
`$$ P(x_1, \dots, x_k | \alpha_1, \dots \alpha_k)=\frac{1}{B(\alpha)} x_1^{\alpha_1 - 1} \dots x_k^{\alpha_k - 1} $$`




???
- Not that important for us
- if $p(x|\theta)$ is multimnomial (discrete distribution), then
  $p(\theta) = \text{Dirichlet}(...)$ is a conjugate prior.

---

# (Block) Gibbs Sampling for LDA
.center[
![:scale 50%](images/lda_params.png)
]

--

`$p(z_{iv} = k \, \vert \, \mathbf{\theta}_i, \phi_k) \propto \exp(\log \theta_{ik} + \log \phi_{k, y_{iv}})$`

`$p(\theta_i \, \vert \, z_{iv} = k, \phi_k) = \text{Dir}(\alpha + \sum_l \mathbb{I}(z_{il} = k))$`

`$p(\phi_k \, \vert \, z_{iv} = k, \mathbf{\theta}_i) = \text{Dir}(\beta + \sum_i \sum_l \mathbb{I}(y_{il} = v, z_{il} = k))$`

???
i is index for documents, v is index for words in document, k index for topics.
z is word-topic assignments.

$z_{iv}=k$ means word v in document i is assigned topic k.

$\theta$ is document topic distribution, $\theta_{ik}$ is the probability of a word in document i to come from topic k.

$y_{iv}$ is the actual word (word-index) of word v in document i.

$\phi_{k, y}$ is the probability of a drawing word y from document k.
$\phi_{k, y_{iv}}$ is the probability 

---

# Collapsed Gibbs Sampling for LDA
.center[
![:scale 50%](images/lda_params.png)
]

Sample only `$p(z_i=k|\mathbf{z}_{-i})$`

Compute $\theta$, $\phi$ from $\mathbf{z}$
???
Now looking at all documents together (i goes over words and documents).
$z_{-i}$ is word-topic assignments for all other words for all documents.
Now sequential problem!
---
# Further Reading

- Rethinking LDA: Why Priors Matter - Hanna Wallach
- LDA Revisited: Entropy, Prior and Convergence –
Zhang et. al.



---
class: middle
# Questions ?


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
	remark.macros['scale'] = function (percentage) {
		//var url = this;
		var base = "https://amueller.github.io/COMS4995-s18/slides/aml-20-040918-topic-models/"
		var url = base + this
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
	};
    config_remark = {
        highlightStyle: 'magula',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
