<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Model Selection and Validation</title>
<meta name="author" content="Robert Utterback (based on slides by Andreas Muller)"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="./revealjs/dist/reveal.css"/>

<link rel="stylesheet" href="./revealjs/dist/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="./revealjs/plugin/highlight/zenburn.css"/><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h2>Model Selection and Validation</h2><h4>09/07/2022</h4>
</section>

<section>
<section id="slide-org81a1c62">
<h2 id="org81a1c62">\(k\)-Nearest Neighbors</h2>

<div id="orga7b3a73" class="figure">
<p><img src="./assets/knn_boundary_test_points.png" alt="knn_boundary_test_points.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Let’s say we have this two-class classification dataset here, with two
features, one on the x axis and one on the y axis. And we have three
new points as marked by the stars here. If I make a prediction using a
one nearest neighbor classifier, what will it predict? It will predict
the label of the closest data point in the training set. That is
basically the simplest machine learning algorithm I can come up with.
</p>

</aside>
</section>
<section id="slide-org892a399">
<h3 id="org892a399"></h3>

<div id="org20c0599" class="figure">
<p><img src="./assets/knn_boundary_k1.png" alt="knn_boundary_k1.png" height="400px" />
</p>
</div>

<p>
\( f(x) = y^{(j)} \) ,
where \(j = \text{argmin}_k ||x_k - x||\)
</p>
<aside class="notes">
<p>
Here's the formula: the prediction for a new x is the y<sub>j</sub> so that x<sub>j</sub>
is the closest point in the training set.  So how to evaluate this
model?
</p>

</aside>
</section>
<section id="slide-org5460699">
<h3 id="org5460699">Accuracy</h3>
<ul>
<li>Split data into training and testing</li>
<li>Evaluate <b>accuracy</b>: # correctly classified / # data points</li>

</ul>
<aside class="notes">
<p>
We talked before about training and testing split.  So we do the split
and then evaluate <b>accuracy</b>, which we define thus for
classification. Just wanted to give the formal definition.
</p>

<p>
But note that we will talk more about classification evaluation later
&#x2013; there is a lot more to say!
</p>

</aside>
</section>
<section id="slide-org5b9ac60">
<h3 id="org5b9ac60">kNN code with scikit-learn</h3>
<div class="org-src-container">

<pre   ><code class="python" >from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = \
	train_test_split(data, target, random_state=0)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)
acc = knn.score(X_test, y_test)
print(f"accuracy: {acc:.2f}")
</code></pre>
</div>
</section>
<section id="slide-org5db0cd2">
<h3 id="org5db0cd2">kNN Notes</h3>
<ul>
<li>Can use more than 1 neighbor: majority vote</li>
<li>Can be used for regression (average "votes")</li>
<li>"fit" doesn't need to do anything &#x2013; just store data</li>

</ul>
</section>
<section id="slide-org93d6192">
<h3 id="org93d6192">kNN Implementation (k=1)</h3>
<div class="org-src-container">

<pre   ><code class="python" >import math
# Assume we have data and target as np
def predict(X):
	predictions = []
	for x in X:
		best = None
		mn = math.inf
		for ngh in data:
			dist = euclidean_distance(d, ngh)
			if dist &lt; mn:
				mn = dist
				best = ngh
		predictions.append(best)
	return np.array(predictions)
</code></pre>
</div>
<aside class="notes">
<p>
Basic, 1-neighbor implementation (Save k neighbors for hw2?)
</p>

<p>
Note: this actually only returns the nearest neighbor. You'd also need
to map this to an actual class based on target.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org266ef97">
<h2 id="org266ef97">Applied kNN</h2>
<div class="outline-text-2" id="text-org266ef97">
</div>
</section>
<section id="slide-org15c39dc">
<h3 id="org15c39dc">kNN for Boston</h3>
<div class="org-src-container">

<pre   ><code class="python" >dataset = load_boston()
df = pd.DataFrame(dataset['data'], columns = dataset['feature_names'])

X_train, X_test, y_train, y_test = \
    train_test_split(df, dataset['target'],
                     test_size=0.20, random_state=42)

from sklearn.neighbors import KNeighborsRegressor
knn = KNeighborsRegressor(n_neighbors=1)
knn.fit(X_train, y_train)
pred = knn.predict(X_test)

print(np.sqrt(mean_squared_error(pred, y_test)))
</code></pre>
</div>

<pre class="example">
6.533458382156615
</pre>
</section>
<section id="slide-org9e7299f">
<h3 id="org9e7299f">Compared to Linear Regression</h3>
<div class="org-src-container">

<pre   ><code class="python" >from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(X_train, y_train)
pred = lm.predict(X_test)

print(np.sqrt(mean_squared_error(pred, y_test)))
</code></pre>
</div>

<pre class="example">
4.9286021826653394
</pre>
</section>
<section id="slide-org57660a8">
<h3 id="org57660a8">Alternative metric: \(R^2\)</h3>
<p>
\( R^2 = \left(1 - \frac{\sum_{i=1}^m (y^{(i)} - \hat{y}^{(i)})^2}{\sum_{i=1}^m (y^{(i)} - \overline{y})^2} \right) \)
</p>
<div class="org-src-container">

<pre   ><code class="python" >print(knn.score(X_test, y_test))
print(lm.score(X_test, y_test))
</code></pre>
</div>

<pre class="example">
0.4179206827765607
0.6687594935356316
</pre>


<aside class="notes">
<p>
Best possible score is 1.0. Can be arbitrarily negative. If you always
predict the mean y, you would get a score of 0.0.
</p>

</aside>
</section>
<section id="slide-orgd39065b">
<h3 id="orgd39065b">kNN for Classification (iris)</h3>
<div class="org-src-container">

<pre   ><code class="python" >from sklearn.datasets import load_iris
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target
X_train, X_test, y_train, y_test = \
    train_test_split(X, y, test_size=0.20, random_state=42)
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
print(knn.score(X_test, y_test))
</code></pre>
</div>

<pre class="example">
1.0
</pre>

</section>
</section>
<section>
<section id="slide-org91cadf2">
<h2 id="org91cadf2">Model Complexity</h2>
<div class="outline-text-2" id="text-org91cadf2">
</div>
</section>
<section id="slide-orgd8a743f">
<h3 id="orgd8a743f">Tuning Parameter: # neighbors</h3>

<div id="orgfaa5139" class="figure">
<p><img src="./assets/knn_boundary_k2.png" alt="knn_boundary_k2.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
So this was the predictions as made by one-nearest neighbor. But we
can also consider more neighbors, for example three. Here is the three
nearest neighbors for each of the points and the corresponding
labels. We can then make a prediction by considering the majority
among these three neighbors. And as you can see, in this case all the
points changed their labels! Clearly the number of neighbors that we
consider matters a lot. But what is the right number?
</p>

<p>
BTW, even k, break ties randomly. Usually just don't use even k though.
</p>

</aside>
</section>
<section id="slide-org6d8d0a2">
<h3 id="org6d8d0a2">kNN with more neighbors</h3>

<div id="org7dec158" class="figure">
<p><img src="./assets/knn_boundary_k3.png" alt="knn_boundary_k3.png" height="400px" />
</p>
</div>

</section>
<section id="slide-org9c7681d">
<h3 id="org9c7681d">Hyperparameters</h3>
<ul>
<li><b>Parameters</b>: values our model learns from data and uses for prediction</li>
<li><b>Hyperparameters</b>: parameters for <i>how</i> our model learns
<ul>
<li>No perfect way to automatically find best</li>

</ul></li>

</ul>
<aside class="notes">
<ul>
<li>In ML we use the data to learn the parameters</li>
<li>In some sense it's like learning which "if" statements to write</li>
<li>But we have these meta-parameters that also determine how we do our learning</li>
<li>knobs, or tuning parameters</li>
<li>These are hyperparameters and we have to find the right away to set them</li>

</ul>

<p>
The is a problem you’ll encounter a lot in machine learning, the
problem of tuning parameters of the model, also called
hyper-parameters, which can not be learned directly from the data.
</p>

</aside>
</section>
<section id="slide-orgddcb2d0">
<h3 id="orgddcb2d0">Influence of Neighbors</h3>

<div id="org57e278d" class="figure">
<p><img src="./assets/knn_boundary_varying_k.png" alt="knn_boundary_varying_k.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Here’s an overview of how the classification changes if we consider
different numbers of neighbors. You can see as red and blue circles
the training data. And the background is colored according to which
class a datapoint would be assigned to for each location. For one
neighbor, you can see that each point in the training set has a little
area around it that would be classified according to it’s label. This
means all the training points would be classified correctly, but it
leads to a very complex shape of the decision boundary. If we increase
the number of neighbors, the boundary between red and blue simplifies,
and with 40 neighbors we mostly end up with a line. This also means
that now many of the training data points would be labeled
incorrectly.
</p>

<p>
Model Complexity vs. Flexibility
</p>

</aside>
</section>
<section id="slide-org10c9e0a">
<h3 id="org10c9e0a">Model Complexity vs. Accuracy</h3>

<div id="orgdf5fd15" class="figure">
<p><img src="./assets/knn_model_complexity.png" alt="knn_model_complexity.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
We can look at this in more detail by comparing training and test set
scores for the different numbers of neighbors. Here, I did a random
75%/25% split again. This is a very noisy plot as the dataset is very
small and I only did a random split, but you can see a trend here. You
can see that for a single neighbor, the training score is 1 so perfect
accuracy, but the test score is only 70%. If we increase the number of
neighbors we consider, the training score goes down, but the test
score goes up, with an optimum at 19 and 21, but then both go down
again.
</p>

<p>
This is a very typical behavior, that I sketched in a schematic for you.
</p>

<p>
Q: Which is more "complex": many neighbors or few? A: Few!
</p>

</aside>
</section>
<section id="slide-orgc9777a8">
<h3 id="orgc9777a8">Model Complexity vs. Error</h3>

<div id="org7f29b92" class="figure">
<p><img src="./assets/overfitting_underfitting_cartoon_train.png" alt="overfitting_underfitting_cartoon_train.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
here is a cartoon version of how this chart looks in general, though it's horizontally flipped to the one with saw for knn. This chart has accuracy on the y axis, and the abstract concept of model complexity on the x axis. If we make our machine learning models more complex, we will get better training set accuracy, as the model will be able to capture more of the variations in the data.
</p>

</aside>
</section>
<section id="slide-org30b5478">
<h3 id="org30b5478">Overfitting and Underfitting</h3>

<div id="org5bf2944" class="figure">
<p><img src="./assets/overfitting_underfitting_cartoon_full.png" alt="overfitting_underfitting_cartoon_full.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
But if we look at the generalization performance, we get a different story. If the model complexity is too low, the model will not be able to capture the main trends, and a more complex model means better generalization. However, if we make the model too complex, generalization performance drops again, because we basically learn to memorize the dataset.
</p>

<p>
If we use too simple a model, this is often called underfitting, while if we use to complex a model, this is called overfitting. And somewhere in the middle is a sweet spot. Most models have some way to tune model complexity, and we’ll see many of them in the next couple of weeks. So going back to nearest neighbors, what parameters correspond to high model complexity and what to low model complexity? high n<sub>neighbors</sub> = low complexity!
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org4a7b082">
<h2 id="org4a7b082">Sources of Error</h2>
<ol>
<li>Noise</li>
<li>Bias</li>
<li>Variance</li>

</ol>
</section>
<section id="slide-org6182c22">
<h3 id="org6182c22">Noise</h3>
<ul>
<li>Data is often inherently noisy</li>
<li>Variables you can't model or predict</li>
<li>Life is messy!</li>

</ul>
<aside class="notes">
<p>
Can try to predict the economy from several key indicators, e.g. stock prices.
But you won't get it exactly correct, there's just too much little detail. You'd have to know everything about the brains of everyone contributing to the economy.
</p>

</aside>
</section>
<section id="slide-org22b34d6">
<h3 id="org22b34d6">Bias</h3>
<ul>
<li>Your model has biased towards certain functions</li>
<li>Over all possible training sets (of size \(m\)), what's the average
fit?</li>
<li>Ex: fitting a constant function -&gt; high bias</li>
<li>low complexity -&gt; high bias</li>

</ul>
<aside class="notes">
<p>
high bias: not very flexible
</p>

</aside>
</section>
<section id="slide-org050ae1c">
<h3 id="org050ae1c">Variance</h3>
<ul>
<li>How much do specific fits vary from expected fits?</li>
<li>i.e., do your specific \(m\) data points affect the fit a lot?</li>
<li>high complexity -&gt; high variance</li>

</ul>
<aside class="notes">
<p>
Example is kNN with low K: decision boundaries will be crazy!
high complexity is going to latch on to noise -&gt; high variance
</p>

</aside>
</section>
<section id="slide-org931922c">
<h3 id="org931922c">Bias-Variance Tradeoff</h3>

<div id="org9ddcc9c" class="figure">
<p><img src="./assets/biasvariance.png" alt="biasvariance.png" height="400px" />
</p>
</div>

<p>
Source: <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">http://scott.fortmann-roe.com/docs/BiasVariance.html</a>
</p>
<aside class="notes">
<p>
We can't actually compute bias and variance, since we don't know the true parameters that generate the data&#x2026;we just get the data and want to make the best predictions
</p>

</aside>
</section>
<section id="slide-org6f989f1">
<h3 id="org6f989f1">Error vs. Dataset Size</h3>

<div id="orgd43213c" class="figure">
<p><img src="./assets/error-vs-data1.png" alt="error-vs-data1.png" height="400px" />
</p>
</div>

<p>
Source: <a href="http://www.stats.ox.ac.uk/~sejdinov/teaching/sdmml15/materials/HT15_lecture12-nup.pdf">http://www.stats.ox.ac.uk/~sejdinov/teaching/sdmml15/materials/HT15_lecture12-nup.pdf</a>
</p>
<aside class="notes">
<p>
For a fixed model complexity&#x2026;
Is this high complexity or low compared to the next?
This one is low
</p>

</aside>
</section>
<section id="slide-org2aa024e">
<h3 id="org2aa024e">Error vs. Dataset Size</h3>

<div id="org4c0e896" class="figure">
<p><img src="./assets/error-vs-data2.png" alt="error-vs-data2.png" height="400px" />
</p>
</div>

<p>
Source: <a href="http://www.stats.ox.ac.uk/~sejdinov/teaching/sdmml15/materials/HT15_lecture12-nup.pdf">http://www.stats.ox.ac.uk/~sejdinov/teaching/sdmml15/materials/HT15_lecture12-nup.pdf</a>
</p>
<aside class="notes">
<p>
This one is high because it <b>really</b> overfits until we get lots of data, then it does better
</p>

<p>
In the limit, the "test" error (which approximates the true error) approaches the training error. True error results from bias + noise in the limit. With just a few points, a fixed complexity model will fit these points well.
</p>

</aside>


</section>
</section>
<section>
<section id="slide-org6f1e87b">
<h2 id="org6f1e87b">Model tuning</h2>
<div class="outline-text-2" id="text-org6f1e87b">
</div>
</section>
<section id="slide-org9e91e09">
<h3 id="org9e91e09">Searching for Hyperparameters</h3>

<div id="org6b3645d" class="figure">
<p><img src="./assets/train_test_set_2d_classification.png" alt="train_test_set_2d_classification.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
So far we’ve work with a split of the data into a training and a test
set, build the model on the training set, and evaluated on the test
set. So now, lets say we want to adjust the parameter n<sub>neigbhbors</sub> in
k neighbors algorithm, how could we do this? [split, try out different
values of k, choose the best on test set] What’s the problem with
that?
</p>

<p>
good for choosing k, overly optimistic for getting accuracy!
You can't use that test set twice!
</p>

</aside>
</section>
<section id="slide-org5fcad26">
<h3 id="org5fcad26">Overfitting the Validation Set</h3>
<div class="org-src-container">

<pre   ><code class="python" >data = load_breast_cancer()
X, y = data.data, data.target
X = scale(X)
X_trainval, X_test, y_trainval, y_test \
	= train_test_split(X, y, random_state=1)
X_train, X_val, y_train, y_val = \
	train_test_split(X_trainval, y_trainval, random_state=1)
knn = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)
print("Validation: {:.3f}".format(knn.score(X_val, y_val)))
print("Test: {:.3f}".format(knn.score(X_test, y_test)))
</code></pre>
</div>

<pre class="example">
Validation: 0.981
Test: 0.944
</pre>


<aside class="notes">
<p>
So I’ll try to illustrate this concept of overfitting the test
set. Basically, the idea is that if you try out too many things on the
test set, you will learn about noise in the test set, and this
knowledge will not generalize to new data.
</p>

<p>
So here I give you an example with the breast cancer dataset that’s
build into scikit-learn. I split the data twice, now I have a
training, a validation and a test set. I build a nearest neighbors
model on the training set, and apply it to the test set and the
validation set. The results are not the same, but they are pretty
close. That they are different is a consequence of the small dataset
and the noisy data.
</p>

</aside>
</section>
<section id="slide-org6805e2b">
<h3 id="org6805e2b">Noisy Tuning</h3>
<div class="org-src-container">

<pre   ><code class="python" >val = []
test = []
for i in range(1000):
    rng = np.random.RandomState(i)
    noise = rng.normal(scale=.1, size=X_train.shape)
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train + noise, y_train)
    val.append(knn.score(X_val, y_val))
    test.append(knn.score(X_test, y_test))
print("Validation: {:.3f}".format(np.max(val)))
print("Test: {:.3f}".format(test[np.argmax(val)]))
</code></pre>
</div>

<pre class="example">
Validation: 0.991
Test: 0.951
</pre>


<aside class="notes">
<p>
So now let me propose a silly way to tune my classifier. I add random
noise to my training set for fitting. And I repeat that 1000 times,
and I check which of these thousand runs has the best performance on
the validation set. That particular run has very high accuracy,
quite a bit better than before. If I would use the same set to
estimate how well my model is, I would think I created a perfect model
and I should write a paper about building much better models by using
noise.
</p>

<p>
But if I test the same model on the test set, I get a lower accuracy
then before: the noise I added was good for the validation set, but
not the test set. The lesson is: If I try something often enough, be
it parameters or noise, at some point I'll get better performance on
the validation set, but that doesn’t mean it will be good for any new
data. By selecting the best among many you introduce a bias, and the
validation accuracy is not a good measure of generalization
performance any more.
</p>

<p>
Similar: coin flip example. Rare to get 20 heads in a row, but flip
\(2^20\) coins..
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org07b20a0">
<h2 id="org07b20a0">Data Splitting</h2>
<div class="outline-text-2" id="text-org07b20a0">
</div>
</section>
<section id="slide-org79b55f9">
<h3 id="org79b55f9">Threefold Split</h3>

<div id="org9603083" class="figure">
<p><img src="./assets/threefold_split.png" alt="threefold_split.png" height="200px" />
</p>
</div>

<ul>
<li>pro: fast, simple</li>
<li>con: high variance, bad use of data for small datasets</li>

</ul>
<aside class="notes">
<p>
The simplest way to combat this overfitting to the test set is by
using a three-fold split of the data, into a training, a validation
and a test set as we just did. We use the training set for model
building, the validation set for parameter selection and the test set
for a final evaluation of the model. So how many models should you try
out on the test set? Only one! Ideally use use the test-set exactly
once, otherwise you make a multiple hypothesis testing error!
</p>

<p>
What are downsides of this? We lose a lot of data for evaluation, and
the results depend on the particular sampling.
</p>

</aside>
</section>
<section id="slide-org3a43430">
<h3 id="org3a43430">Overfitting the Validation Set</h3>

<div id="org55c835c" class="figure">
<p><img src="./assets/overfitting_validation_set_1.png" alt="overfitting_validation_set_1.png" />
</p>
</div>
</section>
<section id="slide-org8327aa7">
<h3 id="org8327aa7">Overfitting the Validation Set</h3>

<div id="orga7c58d9" class="figure">
<p><img src="./assets/overfitting_validation_set_2.png" alt="overfitting_validation_set_2.png" />
</p>
</div>
</section>
<section id="slide-org0d4686e">
<h3 id="org0d4686e">Overfitting the Validation Set</h3>

<div id="org9f82825" class="figure">
<p><img src="./assets/overfitting_validation_set_3.png" alt="overfitting_validation_set_3.png" />
</p>
</div>
</section>
<section id="slide-orgd970e0b">
<h3 id="orgd970e0b">Overfitting the Validation Set</h3>

<div id="org8e2b371" class="figure">
<p><img src="./assets/overfitting_validation_set_4.png" alt="overfitting_validation_set_4.png" />
</p>
</div>
</section>
<section id="slide-orge94aec6">
<h3 id="orge94aec6">Implementing Threefold Split</h3>
<div class="org-src-container">

<pre   ><code class="python" >X_trainval, X_test, y_trainval, y_test = \
	train_test_split(X, y, random_state=1)
X_train, X_val, y_train, y_val = \
	train_test_split(X_trainval, y_trainval, random_state=1)
val_scores = []
neighbors = np.arange(1, 15, 2)
for i in neighbors:
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    val_scores.append(knn.score(X_val, y_val))
print("best validation score: {:.3f}".format(np.max(val_scores)))
best_n_neighbors = neighbors[np.argmax(val_scores)]
print("best n_neighbors: {}".format(best_n_neighbors))
knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)
knn.fit(X_trainval, y_trainval)
print("test-set score: {:.3f}".format(knn.score(X_test, y_test)))
</code></pre>
</div>

<pre class="example">
best validation score: 0.991
best n_neighbors: 3
test-set score: 0.958
</pre>


<aside class="notes">
<p>
Here is an implementation of the three-fold split for selecting the
number of neighbors. For each number of neighbors that we want to try,
we build a model on the training set, and evaluate it on the
validation set. We then pick the best validation set score, here
that’s 99.1%, achieved when using three neighbors. We then retrain the
model with this parameter, and evaluate on the test set. The
retraining step is somewhat optional. We could also just use the best
model. But retraining allows us to make better use of all the data.
</p>

<p>
Still, depending on the test-set size we might be using only 70% or
80% of the data, and our results depend on how exactly we split the
datasets. So how can we make this more robust?
</p>

</aside>
</section>
<section id="slide-orgc69ef26">
<h3 id="orgc69ef26">Methods for Splitting Data</h3>
<div class="column" style="float:left; width: 50%">
<p>
Nonrandom
</p>
<ul>
<li>create model from patients in same disease stage, test on different sample population</li>
<li>spam filtering: care more about current spam techniques than old ones</li>

</ul>
</div>
<div class="column" style="float:left; width: 50%">
<p>
Random
</p>
<ul>
<li>Simple random sample</li>
<li>Stratified random sampling</li>
<li>Maximum dissimilarity sampling</li>

</ul>
</div>
<aside class="notes">
<p>
Let's just talk briefly about splitting data. <b>How</b> you split the data
can influence how accurate your results are!
</p>

<p>
Nonrandom: (1) want to test how well it generalizes to other
populations, (2) want to sample the right spam
</p>

<p>
But usually we want something that is random in some way. Actually,
what we really care about is splitting into <b>similar</b> data sets.
</p>

<p>
Simple random does not control for data attributes, e.g., one class is
represented more than the others!
</p>

</aside>
</section>
<section id="slide-orgc66d176">
<h3 id="orgc66d176">Stratified Random Sampling</h3>
<ul>
<li>Random sample within subgroups (e.g., classes)</li>
<li>Similar strategy for numeric values: break into groups (e.g., low,
medium, high), sample from these</li>

</ul>
</section>
<section id="slide-org1534604">
<h3 id="org1534604">Maximum Dissimilarity Sampling</h3>
<ul>
<li>Pick some measure of "dissimilarity", e.g., distance</li>
<li>Repeatedly pick data points that are most dissimilar to current set
of points</li>
<li>Requires measure of distance and method to determine dissimilarity
between sets of points</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgf7af965">
<h2 id="orgf7af965">Cross Validation</h2>
<div class="outline-text-2" id="text-orgf7af965">
</div>
</section>
<section id="slide-orgaf333d1">
<h3 id="orgaf333d1">Resampling Methods</h3>
<ul>
<li>Use subset of samples to fit model, everything else to estimate error</li>
<li>Repeat over and over with new samples</li>
<li>Aggregate results</li>
<li>Different methods in choosing subsamples</li>

</ul>
<aside class="notes">
<p>
But sometimes we simply don't have enough data to split into train,
test, and val sets! Luckily we have resampling methods. We're mainly
concerned with cross-validation
</p>

</aside>
</section>
<section id="slide-org396b31f">
<h3 id="org396b31f">Cross Validation</h3>

<div id="org409ee70" class="figure">
<p><img src="./assets/kfold_cv.png" alt="kfold_cv.png" height="200px" />
</p>
</div>
<ul>
<li class="fragment appear">pro: more stable, more data</li>
<li class="fragment appear">con: slower</li>

</ul>
<aside class="notes">
<p>
The answer is of course cross-validation. In cross-validation, you
split your data into multiple folds, usually 5 or 10, and built
multiple models. You start by using fold1 as the test data, and the
remaining ones as the training data. You build your model on the
training data, and evaluate it on the test fold. For each of the
splits of the data, you get a model evaluation and a score. In the
end, you can aggregate the scores, for example by taking the
mean. What are the pros and cons of this? Each data point is in the
test-set exactly once! Takes 5 or 10 times longer! Better data use
(larger training sets). Does that solve all problems? No, it replaces
only one of the splits, usually the inner one!
</p>

</aside>
</section>
<section id="slide-orgcd4671f">
<h3 id="orgcd4671f">Cross Validation Workflow</h3>

<div id="orgf1cd9d9" class="figure">
<p><img src="./assets/grid_search_cross_validation.png" alt="grid_search_cross_validation.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Here is how the workflow looks like when we are using five-fold
cross-validation together with a test-set split for adjusting
parameters. We start out by splitting off the test data, and then we
perform cross-validation on the training set. Once we found the right
setting of the parameters, we retrain on the whole training set and
evaluate on the test set.
</p>

</aside>
</section>
<section id="slide-org947254a">
<h3 id="org947254a">Grid Search</h3>
<div class="org-src-container">

<pre   ><code class="python" >from sklearn.model_selection import cross_val_score
X_train, X_test, y_train, y_test = train_test_split(X, y)
neighbors = np.arange(1, 15, 2)
cross_val_scores = []
for i in neighbors:
    knn = KNeighborsClassifier(n_neighbors=i)
    scores = cross_val_score(knn, X_train, y_train, cv=10)
    cross_val_scores.append(np.mean(scores))
print("best cross-validation score: {:.3f}".format(np.max(cross_val_scores)))
best_n_neighbors = neighbors[np.argmax(cross_val_scores)]
print("best n_neighbors: {}".format(best_n_neighbors))
knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)
knn.fit(X_train, y_train)
print("test-set score: {:.3f}".format(knn.score(X_test, y_test)))
</code></pre>
</div>

<pre class="example">
best cross-validation score: 0.967
best n_neighbors: 5
test-set score: 0.944
</pre>

<aside class="notes">
<p>
Here is an implementation of this for k nearest neighbors.
</p>

<p>
We split the data, then we iterate over all parameters and for each of them we do cross-validation.
</p>

<p>
We had seven different values of n<sub>neighbors</sub>, and we are running 10
fold cross-validation. How many models to we train in total? 10 * 7 +
1 = 71 (the one is the final model)
</p>

</aside>
</section>
<section id="slide-orgf583687">
<h3 id="orgf583687">Model Evaluation Workflow</h3>

<div id="org065395d" class="figure">
<p><img src="./assets/gridsearch_workflow.png" alt="gridsearch_workflow.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Here is a conceptual overview of this way of tuning parameters, we
start of with the dataset and a candidate set of parameters we want to
try, labeled parameter grid, for example the number of neighbors.
</p>

<p>
We split the dataset in to training and test set. We use
cross-validation and the parameter grid to find the best
parameters. We use the best parameters and the training set to build a
model with the best parameters, and finally evaluate it on the test
set.
</p>

<p>
&#x2026;now other cross validation strategies
</p>

</aside>
</section>
<section id="slide-orgbad740e">
<h3 id="orgbad740e">Nested Cross Validation</h3>
<ul>
<li>Replace outer split by CV loop</li>
<li>Doesn't yield single model (inner loop might have different best parameter settings)</li>
<li>Takes a long time, not that useful in practice</li>
<li>Tells you how good your hyperparameter search is</li>

</ul>
<aside class="notes">
<p>
We could additionally replace the outer split of the data by
cross-validation. That would yield what’s known as nested
cross-validation. This is sometimes interesting when comparing
different models, but it will not actually yield one final model. It
will yield one model for each loop of the outer fold, which might have
different settings of the parameters. Also, this takes a really long
time to train, by an additional factor of 5 or 10, so this is not used
very commonly in practice.
</p>

<p>
But let’s dive into the cross-validation a bit more.
</p>

</aside>
</section>
<section id="slide-orge1b4dcd">
<h3 id="orge1b4dcd">Choosing \(k\)</h3>
<ul>
<li>Usually 5 or 10</li>
<li>No formal rule</li>
<li>smaller \(k \implies\) worse estimate of error</li>
<li>But smaller \(k\) is more computational efficient</li>

</ul>
<aside class="notes">
<p>
As \(k\) gets larger, difference in size between training set and the
resampled subsets gets smaller
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org923cccb">
<h2 id="org923cccb">Cross-Validation Strategies</h2>
<div class="outline-text-2" id="text-org923cccb">
</div>
</section>
<section id="slide-org1f48d30">
<h3 id="org1f48d30">Stratified KFold</h3>

<div id="org93e54d4" class="figure">
<p><img src="./assets/stratified_cv.png" alt="stratified_cv.png" height="400px" />
</p>
</div>

<p>
Stratified: Ensure relative class frequencies in each fold reflect
relative class frequencies on the whole dataset.
</p>
<aside class="notes">
<p>
The idea behind stratified k-fold cross-validation is that you want
the test set to be as representative of the dataset as
possible. StratifiedKFold preserves the class frequencies in each fold
to be the same as of the overall dataset. Here is and example of a
dataset with three classes that are ordered. If you apply standard
three-fold to this, the first third of the data would be in the first
fold, the second in the second fold and the third in the third
fold. Because this data is sorted, that would be particularly bad. If
you use stratified cross-validation it would make sure that each fold
has exactly 1/3 of the data from each class.
</p>

<p>
This is also helpful if your data is very imbalanced. If some of the
classes are very rare, it could otherwise happen that a class is not
present at all in a particular fold.
</p>

</aside>

</section>
<section id="slide-org25bd1dc">
<h3 id="org25bd1dc">Scikit-learn defaults</h3>
<ul>
<li>Five-fold is default number of folds</li>
<li>For classification cross-validation is stratified</li>
<li><code>train_test_split</code> has stratify option:</li>

</ul>
<p>
<code class="src src-python">train_test_split(X, y, stratify=y)</code>
</p>
<ul>
<li>No shuffle by default!</li>

</ul>
<aside class="notes">
<p>
Before we go to the other strategies, I wanted to point out the
default behavior in scikit-learn. By default, all cross-validation
strategies are five-fold. If you do cross-validation for
classification, it will be stratified by default. Because of how the
interface is done, that’s not true for train<sub>test</sub><sub>split</sub> and if you
want a stratified train<sub>test</sub><sub>split</sub>, which is always a good idea, you
should use stratify=y Another thing that’s important to keep in mind
is that by default scikit-learn doesn’t shuffle! So if you run
cross-validation twice with the default parameters, it will yield
exactly the same results.
</p>

</aside>
</section>
<section id="slide-org9f901ef">
<h3 id="org9f901ef">Repeated KFold and Leave-One-Out</h3>
<ul>
<li>LeaveOneOut: <code class="src src-python">KFold(n_folds=n_samples)</code> 
<ul>
<li>High variance</li>
<li>takes a long time</li>

</ul></li>
<li>Better: RepeatedKFold
<ul>
<li>Apply KFold or StratifiedKFold multiple times with shuffled data.</li>
<li>Reduces variance!</li>

</ul></li>

</ul>
<aside class="notes">
<p>
If you want even better estimates of the generalization performance,
you could try to increase the number of folds, with the extreme of
creating one fold per sample. That’s called “LeaveOneOut
cross-validation”. However, because the test-set is so small every
time, and the training sets all have very large overlap, this method
has very high variance. A better way to get a robust estimate is to
run 5-fold or 10-fold cross-validation multiple times, while shuffling
the dataset.
</p>

<p>
Note that variance here is about this technique's estimate of
generalization performance (error), NOT the variance of the model.
</p>

</aside>
</section>
<section id="slide-orgb0bc38f">
<h3 id="orgb0bc38f">Shuffle Split</h3>

<div id="org5b8617e" class="figure">
<p><img src="./assets/shuffle_split_cv.png" alt="shuffle_split_cv.png" height="200px" />
</p>
</div>
<aside class="notes">
<p>
Another interesting variant is shuffle split and stratified shuffle
split. In shuffle split, we repeatedly sample disjoint training and
test sets randomly. You only have to specify the number of iterations,
the training set size and the test set size. This also allows you to
run many iterations with reasonably large test-sets. It’s also great
if you have a very large training set and you want to subsample it to
get quicker results.
</p>

</aside>
</section>
<section id="slide-orgb832b28">
<h3 id="orgb832b28">Group KFold</h3>

<div id="org7d43452" class="figure">
<p><img src="./assets/group_kfold.png" alt="group_kfold.png" height="200px" />
</p>
</div>
<aside class="notes">
<p>
A somewhat more complicated approach is group k-fold. This is actually
for data that doesn’t fulfill our IID assumption and has correlations
between samples. The idea is that there are several groups in the data
that each contain highly correlated samples. You could think about
patient data where you have multiple samples for each patient, then
the groups would be which patient a measurement was taken from. If you
want to know how well your model generalizes to new patients, you need
to ensure that the measurements from each patient are either all in
the training set, or all in the test set. And that’s what GroupKFold
does.
</p>

<p>
In this example, there are four groups, and we want three folds. The
data is divided such that each group is contained in exactly one
fold. There are several other cross-validation methods in scikit-learn
that use these groups.
</p>

</aside>
</section>
<section id="slide-org540c454">
<h3 id="org540c454">Time Series Split</h3>

<div id="org6ca90c8" class="figure">
<p><img src="./assets/time_series_cv.png" alt="time_series_cv.png" height="300px" />
</p>
</div>
<aside class="notes">
<p>
Another common case of data that’s not independent is time
series. Usually todays stock price is correlated with yesterdays and
tomorrows. If you randomly split time series, this makes predictions
deceivingly simple. In applications, you usually have data up to some
point, and then try to make predictions for the future, in other
words, you’re trying to make a forecast. The TimeSeriesSplit in
scikit-learn simulates that, by taking increasing chunks of data from
the past and making predictions on the next chunk. This is quite
different from the other was to do cross-validation, in that the
training sets are all overlapping, but it’s more appropriate for
time-series.
</p>

</aside>
</section>
<section id="slide-org80b33e0">
<h3 id="org80b33e0">Using CV Generators</h3>
<div class="org-src-container">

<pre   ><code class="python" >from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit
kfold = KFold(n_splits=5)
skfold = StratifiedKFold(n_splits=5, shuffle=True)
ss = ShuffleSplit(n_splits=20, train_size=.4, test_size=.3)
print("KFold:\n{}".format(
      cross_val_score(KNeighborsClassifier(), X, y, cv=kfold)))
print("StratifiedKFold:\n{}".format(
      cross_val_score(KNeighborsClassifier(), X, y, cv=skfold)))
print("ShuffleSplit:\n{}".format(
      cross_val_score(KNeighborsClassifier(), X, y, cv=ss)))
</code></pre>
</div>

<pre class="example">
KFold:
[0.92982456 0.95614035 0.96491228 0.98245614 0.96460177]
StratifiedKFold:
[0.97368421 0.98245614 0.98245614 0.92982456 0.97345133]
ShuffleSplit:
[0.97076023 0.97076023 0.95321637 0.95906433 0.97076023 0.92397661
 0.95906433 0.94736842 0.95906433 0.98245614 0.96491228 0.95321637
 0.96491228 0.94152047 0.95906433 0.95321637 0.95906433 0.93567251
 0.94736842 0.97076023]
</pre>


<aside class="notes">
<p>
Ok, so how do we use these cross-validation generators? We can simply
pass the object to the cv parameter of the cross<sub>val</sub><sub>score</sub> function,
instead of passing a number. Then that generator will be used. Here
are some examples for k-neighbors classifier. We instantiate a Kfold
object with the number of splits equal to 5, and then pass it to
cross<sub>val</sub><sub>score</sub>. We can do the same with StratifiedKFold, and we can
also shuffle if we like, or we can use Shuffle split.
</p>

</aside>
</section>
<section id="slide-org576be56">
<h3 id="org576be56"></h3>

<div id="orgf3d73a9" class="figure">
<p><img src="./assets/gridsearch_workflow.png" alt="gridsearch_workflow.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Let’s come back to the general workflow for adjusting
hyper-parameters, though. So we start with hyper parameters we want to
adjust and our dataset, we split it into training and test set, find
the best parameters using cross-validation, retrain the model and then
do a final evaluation on the test set. Because this is such a common
pattern, there is a helper class for this in scikit-learn, called
GridSearch CV, which does most of these steps for you.
</p>

</aside>
</section>
<section id="slide-org58e8b4a">
<h3 id="org58e8b4a">Grid Search CV</h3>
<div class="org-src-container">

<pre   ><code class="python" >from sklearn.model_selection import GridSearchCV
X_train, X_test, y_train, y_test = \
	train_test_split(X, y, stratify=y)
param_grid = {'n_neighbors':  np.arange(1, 15, 2)}
grid = GridSearchCV(KNeighborsClassifier(),
					param_grid=param_grid,
					cv=10, return_train_score=True)
grid.fit(X_train, y_train)
print("best mean cross-validation score: {:.3f}".format(grid.best_score_))
print("best parameters: {}".format(grid.best_params_))
print("test-set score: {:.3f}".format(grid.score(X_test, y_test)))
</code></pre>
</div>

<pre class="example">
best mean cross-validation score: 0.969
best parameters: {'n_neighbors': 5}
test-set score: 0.965
</pre>


<aside class="notes">
<p>
Here is an example. We still need to split our data into training and
test set. We declare the parameters we want to search over as a
dictionary. In this example the parameter is just n<sub>neighbors</sub> and the
values we want to try out are a range. The keys of the dictionary are
the parameter names and the values are the parameter settings we want
to try. If you specify multiple parameters, all possible combinations
are tried. This is where the name grid-search comes from - it’s an
exhaustive search over all possible parameter combinations that you
specify.
</p>

<p>
GridSearchCV is a class, and it behaves just like any other model in
scikit-learn, with a fit, predict and score method. It’s what we call
a meta-estimator, since you give it one estimator, here the
KneighborsClassifier, and from that GridSearchCV constructs a new
estimator that does the parameter search for you. You also specify the
parameters you want to search, and the cross-validation strategy. Then
GridSearchCV does all the other things we talked about, it does the
cross-validation and parameter selection, and retrains a model with
the best parameter settings that were found. We can check out the best
cross-validation score and the best parameter setting with the
bestscore and bestparams attributes. And finally we can compute the
accuracy on the test set, simply but using the score method! That will
use the retrained model under the hood.
</p>

</aside>
</section>
<section id="slide-orge3880b6">
<h3 id="orge3880b6">kNN Search Results</h3>
<div class="org-src-container">

<pre   ><code class="python" >import pandas as pd
results = pd.DataFrame(grid.cv_results_)
print(results.columns)
print(results.params)
</code></pre>
</div>

<pre class="example" id="org56ee6ea">
Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',
       'param_n_neighbors', 'params', 'split0_test_score', 'split1_test_score',
       'split2_test_score', 'split3_test_score', 'split4_test_score',
       'split5_test_score', 'split6_test_score', 'split7_test_score',
       'split8_test_score', 'split9_test_score', 'mean_test_score',
       'std_test_score', 'rank_test_score', 'split0_train_score',
       'split1_train_score', 'split2_train_score', 'split3_train_score',
       'split4_train_score', 'split5_train_score', 'split6_train_score',
       'split7_train_score', 'split8_train_score', 'split9_train_score',
       'mean_train_score', 'std_train_score'],
      dtype='object')
0     {'n_neighbors': 1}
1     {'n_neighbors': 3}
2     {'n_neighbors': 5}
3     {'n_neighbors': 7}
4     {'n_neighbors': 9}
5    {'n_neighbors': 11}
6    {'n_neighbors': 13}
Name: params, dtype: object
</pre>

<aside class="notes">
<p>
GridSearchCV also computes a lot of interesting statistics for you,
which are stored in the cvresults attribute. That attribute is a
dictionary, but it’s easiest to convert it to a pandas dataframe to
look at it. Here you can see the columns. Theres mean fit time, mean
score time, mean test scores, mean training scores, standard
deviations and scores for each individual split of the data. And there
is one row for each setting of the parameters we tried out.
</p>

</aside>
</section>
<section id="slide-orgf64f3df">
<h3 id="orgf64f3df">kNN Search Results</h3>

<div id="org3e181a6" class="figure">
<p><img src="./assets/grid_search_n_neighbors.png" alt="grid_search_n_neighbors.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
We can use this for example to plot the results of cross-validation
over the different parameters. Here are the mean training score and
mean test score together with one standard deviation.
</p>

</aside>
</section>
</section>
</div>
</div>
<script src="./revealjs/dist/reveal.js"></script>
<script src="./revealjs/plugin/highlight/highlight.js"></script>
<script src="./revealjs/plugin/notes/notes.js"></script>


<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
plugins: [RevealHighlight, RevealNotes],
slideNumber:true
});

</script>
</body>
</html>
