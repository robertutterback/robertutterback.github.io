<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Dimensionality Reduction</title>
<meta name="author" content="Robert Utterback (based on slides by Andreas Muller)"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="./revealjs/dist/reveal.css"/>

<link rel="stylesheet" href="./revealjs/dist/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="./revealjs/plugin/highlight/zenburn.css"/><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h2>Dimensionality Reduction</h2><h4>10/31/2022</h4><h5>Robert Utterback (based on slides by Andreas Muller)</h5>
</section>
\(
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathop{\boldsymbol{E}}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\ltwo}[1]{\norm{#1}_2}
\newcommand{\lone}[1]{\norm{#1}_1}
\newcommand{\sgn}[1]{\text{sign}\left( #1 \right)}
\newcommand{\e}{\mathrm{e}}
\newcommand{\minw}{\min_{\vec{w} \in \mathbb{R}^p, b\in\mathbb{R}}}
\newcommand{\summ}{\sum_{i=1}^m}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\logloss}{\ln{(\exp{(-y^{(i)}(\vec{w}^T\vec{x}^{(i)}+b))} + 1)}}
\newcommand{\ai}{\alpha^{(i)}}
\newcommand{\w}{\vec{w}}
\newcommand{\wt}{\vec{w}^T}
\newcommand{\xi}{\vec{x}^{(i)}}
\newcommand{\xit}{(\vec{x}^{(i)})^T}
\newcommand{\xip}{\xit \vec{w}}
\newcommand{\tip}{(\phi(\xi)^T \phi(\vec{x}))}
\)

<section>
<section id="slide-orga0f52af">
<h2 id="orga0f52af">Dimensionality Reduction</h2>
<aside class="notes">
<ul>
<li>PCA, Discriminants, Manifold Learning</li>
<li>Main idea is to take a high-dimensional data set (Q: which means what?)</li>
<li>And reduce the number of dimensions</li>
<li>Either for visualization or for an ML pipeline</li>

</ul>

</aside>
</section>
</section>
<section>
<section id="slide-org7bf5d8e">
<h2 id="org7bf5d8e">Principal Component Analysis</h2>
<aside class="notes">
<p>
First one we're going to talk about
</p>

</aside>
</section>
<section id="slide-org93ddc26">
<h3 id="org93ddc26">Principal Component Analysis</h3>

<div id="orgf9eccba" class="figure">
<p><img src="./assets/pca-intuition.png" alt="pca-intuition.png" height="400px" />
</p>
</div>
<aside class="notes">
<ul>
<li>A 2-dimensional input space</li>
<li>PCA finds the directions of maximum variance: the direction most elongated</li>
<li>Then look for the component that captures second-most variance, and so on</li>
<li>Returns an orthogonal basis (linear combinations of features with
restrictions)</li>
<li>We could revisualize so that the components are our axes</li>
<li>That's what the transformed data is</li>
<li>So far just a rotation of input space</li>
<li>Color is not showing anything about the samples, it is just to show
you where the data "goes" during the transformations</li>
<li>Since they're ordered by decreasing variance, we can drop some of these components</li>
<li>e.g., drop the second since it's not as important here</li>
<li>So we've gone from 2D to 1D</li>
<li>Kind of like the opposite of adding polynomial features: we're
noticing when the dataset is actually in a lower dimensional space
and reducing it down</li>
<li>We can then rotate back to the original space, which is basically
removing some information</li>

</ul>

</aside>
</section>
<section id="slide-orgfd18f9a">
<h3 id="orgfd18f9a">PCA objective(s)</h3>
<div class="column" style="float:left; width: 50%">
<font size=6>
<p>
\[\large\max\limits_{u_1 \in R^p, \| u_1 \| = 1} \text{var}(Xu_1)\]
\[\large\max\limits_{u_1 \in R^p, \| u_1 \| = 1} u_1^T \text{cov} (X) u_1\]
</p>
</font>
</div>
<div class="column" style="float:right; width: 50%">

<div id="orgdd1f733" class="figure">
<p><img src="./assets/pca-intuition.png" alt="pca-intuition.png" />
</p>
</div>
</div>
<aside class="notes">
<ul>
<li>Formalized like this: look over all vectors with norm 1, what is the
variance of projecting the data along that axis, that's max component</li>
<li>Could be rewritten with second equation</li>
<li>Then subtract u_1 from data and iterate to get u_2, etc.</li>
<li>Note: Direction of u_1 or any u doesn't matter; show how components
could face other direction.</li>
<li>Direction, not sign. Sign is meaningless.</li>

</ul>

</aside>
</section>
<section id="slide-org8d3aac7">
<h3 id="org8d3aac7">PCA objective(s)</h3>
<font size=6>
<p>
\[\large\min_{X', \text{rank}(X') = r}\|X-X'\|\]
</p>
</font>


<div id="org6b9fa25" class="figure">
<p><img src="./assets/pca-intuition.png" alt="pca-intuition.png" height="300px" />
</p>
</div>
<aside class="notes">
<ul>
<li>Second way of looking at it: reconstruction error.</li>
<li>How well does the "reconstruction" (transformed, dropped, rotated
back) match the original space?</li>
<li>So look at the difference between original data and reconstructed</li>
<li>Just a different formulation of the same problem</li>

</ul>

</aside>
</section>
<section id="slide-orge2894a1">
<h3 id="orge2894a1">PCA Computation</h3>
<ul>
<li>Center X (subtract mean).</li>
<li>In practice: Also scale to unit variance.</li>
<li>Compute singular value decomposition:</li>

</ul>

<div id="org55fc5c3" class="figure">
<p><img src="./assets/pca-computation.png" alt="pca-computation.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>First centers the data, also scale the variance</li>
<li>Then it does this SVD, you might remember from Linear Algebra</li>
<li>The values in the diagonals are the variance for each of the
components in V.</li>
<li>Additional processing: use D to rescale the data in rotated space so
variance is same across each of the new dimensions</li>

</ul>

</aside>
</section>
<section id="slide-org4abe754">
<h3 id="org4abe754">Whitening</h3>

<div id="orged04bf7" class="figure">
<p><img src="./assets/whitening.png" alt="whitening.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>Basically rotate data, then scale to stddev. 1</li>
<li>Same as using PCA without whitening, then doing StandardScaler.</li>
<li>You can see now the variance, visually looks like a ball, it was a
Gaussian before. In the context of signal processing, in general,
this is called whitening (white noise).</li>
<li>And if you want to use PCA as a feature transformation, to extract
features you might want to do that, because otherwise, the magnitude
of the first principal component will be much bigger than the
magnitude of the other components since you just rotated it. So the
small variance directions will have only very small variance because
that's how you define them.</li>
<li>The question here is do you think the principal components are of
similar importance? Or do you think the original feature is of
similar importance?</li>
<li>And if you think the principal components are of similar important
then you should do this. So if you want to go on and put this in a
classifier, this might be helpful. But I don't think I can actually
give you a clear rule when you would want to do this or when not
to. But it kind of makes senses if you think of it as being centered
scaler in a transformed space that seems like something that might
be helpful for a classifier.</li>

</ul>

</aside>
</section>
</section>
<section>
<section id="slide-org515272f">
<h2 id="org515272f">PCA for Visualization</h2>
<div class="outline-text-2" id="text-org515272f">
</div>
</section>
<section id="slide-orge595ac6">
<h3 id="orge595ac6">PCA for Visualization</h3>
<div class="column" style="float:left; width: 50%">
<font size=5>
<div class="org-src-container">

<pre   ><code class="python" >from sklearn.decomposition import PCA
print(cancer.data.shape)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(cancer.data)
# print(X_pca.shape)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cancer.target)
plt.xlabel("first principal component")
plt.ylabel("second principal component")
components = pca.components_
plt.imshow(components.T)
plt.yticks(range(len(cancer.feature_names)), cancer.feature_names)
plt.colorbar()
</code></pre>
</div>
</font>


<div id="org6959679" class="figure">
<p><img src="./assets/pca-for-visualization-cancer-data.png" alt="pca-for-visualization-cancer-data.png" height="250px" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">

<div id="org2f0be41" class="figure">
<p><img src="./assets/pca-for-visualization-components-color-bar.png" alt="pca-for-visualization-components-color-bar.png" />
</p>
</div>
</div>
<aside class="notes">
<ul>
<li>Example on a real dataset: breast cancer dataset</li>
<li>In this case we know the labels, so you're basically doing this to
visualize and maybe decide whether this is easy or hard to separate</li>
<li>Looks like it's relatively easy to separate; probably need a little nonlinearity</li>
<li>Can also visualize the components: basically it's just looking at two features</li>
<li>i.e., the components are just combinations of two features</li>
<li>I didn't scale my data; Q: Why am I getting this?</li>
<li>A: b/c I didn't scale, these two features have larger scales and dominate</li>
<li>Imagine one feature with very large scale. Without scaling, it’s
guaranteed to be the first principal component!</li>
<li>This is A LOT easier than looking at a 30x30 heatmap to decide which
features are important!</li>

</ul>

</aside>
</section>
<section id="slide-org89348a7">
<h3 id="org89348a7">Scaling</h3>
<div class="org-src-container">

<pre   ><code class="python" >pca_scaled = make_pipeline(StandardScaler(),
                       PCA(n_components=2),
                       LogisticRegression(C=10000))
pca_scaled.fit(X_train, y_train)
print(pca_scaled.score(X_train, y_train))
print(pca_scaled.score(X_test, y_test))
</code></pre>
</div>


<div id="orgc1290d3" class="figure">
<p><img src="./assets/scaled-pca-for-visualization-cancer-data.png" alt="scaled-pca-for-visualization-cancer-data.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>Now with scaling, I get a better idea of the dataset</li>

</ul>

</aside>
</section>
<section id="slide-org5a92eef">
<h3 id="org5a92eef">Inspecting Components</h3>
<div class="org-src-container">

<pre   ><code class="python" >components = pca_scaled.named_steps['pca'].components_
plt.imshow(components.T)
plt.yticks(range(len(cancer.feature_names)),
           cancer.feature_names)
plt.colorbar()
</code></pre>
</div>
<div class="column" style="float:left; width: 50%">

<div id="orgf3ff292" class="figure">
<p><img src="./assets/inspecting-pca-scaled-components.png" alt="inspecting-pca-scaled-components.png" height="350px" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">

<div id="org347aaab" class="figure">
<p><img src="./assets/inspecting-pca-scaled-components-2.png" alt="inspecting-pca-scaled-components-2.png" height="350px" />
</p>
</div>
</div>
<aside class="notes">
<ul>
<li>Now we see the PC are made up of most of the features</li>
<li>First column is first PC, second is second</li>
<li>If you're an expert in the area you'd be able to interpret these values</li>
<li>Can also do a scatter plot, though only works if you're only using 2 PCs</li>

</ul>

</aside>
</section>
</section>
<section>
<section id="slide-org66471fe">
<h2 id="org66471fe">PCA for Regularization</h2>
<div class="outline-text-2" id="text-org66471fe">
</div>
</section>
<section id="slide-orgbcbebd2">
<h3 id="orgbcbebd2">PCA for Regularization</h3>
<div class="org-src-container">

<pre   ><code class="python" >X_train, X_test, y_train, y_test = \
    train_test_split(cancer.data, cancer.target,
                     stratify=cancer.target, random_state=0)
lr = LogisticRegression(C=10000, solver='liblinear')
lr.fit(X_train, y_train)
print(f"{lr.score(X_train, y_train):.3f}")
print(f"{lr.score(X_test, y_test):.3f}")
</code></pre>
</div>

<pre class="example">
0.984
0.930
</pre>


<div class="org-src-container">

<pre   ><code class="python" >from sklearn.decomposition import PCA
lr = LogisticRegression(C=10000, solver='liblinear')
pca_lr = make_pipeline(StandardScaler(),
                       PCA(n_components=2), lr)
pca_lr.fit(X_train, y_train)
print(f"{pca_lr.score(X_train, y_train):.3f}")
print(f"{pca_lr.score(X_test, y_test):.3f}")
</code></pre>
</div>

<pre class="example">
0.960
0.923
</pre>


<aside class="notes">
<ul>
<li>PCA can also be used for regularization</li>
<li>Remove some features, then train &#x2014; avoids overfitting</li>
<li>So here I'm basically removing regularization from
LogisticRegression by setting C to a very high value</li>
<li>Going to use PCA for regularization</li>
<li>First I just use LR, definitely overfitting judging by train vs. test score</li>
<li>Reduces overfitting a bit here, though this is a simple problem</li>
<li>In general it may not be so good with just 2 components</li>
<li>You could do grid search to find the best # components!</li>

</ul>

</aside>
</section>
<section id="slide-org46e5d3a">
<h3 id="org46e5d3a">Variance Covered</h3>

<div id="org698f03c" class="figure">
<p><img src="./assets/variance-covered.png" alt="variance-covered.png" height="350px" />
</p>
</div>
<div class="org-src-container">

<pre   ><code class="python" >pca_lr = make_pipeline(StandardScaler(),
                       PCA(n_components=6), lr)
pca_lr.fit(X_train, y_train)
print(f"{pca_lr.score(X_train, y_train):.3f}")
print(f"{pca_lr.score(X_test, y_test):.3f}")
</code></pre>
</div>

<pre class="example">
0.981
0.958
</pre>


<aside class="notes">
<ul>
<li>Could obviously also do cross-validation + grid-search</li>
<li>Here I'm plotting how much variance of the data is explained by each
components</li>
<li>So you can look at this and maybe decide how many components to keep</li>
<li>Maybe here I decide to use 6 components</li>
<li>Slightly better result and slightly less overfitting</li>
<li>This is a nice technique</li>
<li>More theoretically sound than regularization</li>
<li>Though it seems like it is possibly throwing away a lot of info!</li>
<li>Note: same plots, different scale. Logscale on bottom</li>

</ul>

</aside>
</section>
<section id="slide-org5a3959a">
<h3 id="org5a3959a">Interpreting Coefficients</h3>
<div class="org-src-container">

<pre   ><code class="python" >pca = pca_lr.named_steps['pca']
lr = pca_lr.named_steps['logisticregression']
coef_pca = pca.inverse_transform(lr.coef_)
</code></pre>
</div>
<div class="column" style="float:left; width: 50%">

<div id="org31b6e24" class="figure">
<p><img src="./assets/PCA+logreg.png" alt="PCA+logreg.png" height="300px" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">

<div id="org6c9ddb4" class="figure">
<p><img src="./assets/logreg+noPCA.png" alt="logreg+noPCA.png" height="300px" />
</p>
</div>
</div>
<aside class="notes">
<ul>
<li>Also can still interpret the coefficients</li>
<li>Take the coefficients, which are weights for each PCA component</li>
<li>Then transform back into original feature space</li>
<li>LHS: coefficient index axis is original features, i.e., how do PCA
feature components use these features</li>
<li>RHS is plotting coefficients from plain to PCA, showing they are related</li>
<li>Note the "named_steps" stuff here</li>

</ul>

</aside>
</section>
<section id="slide-org1e25f6f">
<h3 id="org1e25f6f">PCA is Unsupervised!</h3>
<div class="column" style="float:left; width: 50%">

<div id="orgeb1cc42" class="figure">
<p><img src="./assets/pca-is-unsupervised-1.png" alt="pca-is-unsupervised-1.png" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">

<div id="org9f73323" class="figure">
<p><img src="./assets/pca-is-unsupervised-2.png" alt="pca-is-unsupervised-2.png" />
</p>
</div>
</div>
<aside class="notes">
<ul>
<li>Here's a case where PCA doesn't work well for supervised learning</li>
<li>This is a 3D classification problem visualized in 2D</li>
<li>PCs on right</li>
<li>Note 3rd PC perfectly classifies the dataset!</li>
<li>But if we just chose the first 2 components, we wouldn't do well at all!</li>
<li>The point is that choosing components based on the variance might
not be meaningful.</li>
<li>B/c you might have features that have very small variance, but that
very small separation is key for classification!</li>
<li>Small variance doesn't mean small importance!</li>

</ul>

</aside>
</section>
</section>
<section>
<section id="slide-orgc3f2210">
<h2 id="orgc3f2210">PCA for Feature Extraction</h2>
<div class="outline-text-2" id="text-orgc3f2210">
</div>
</section>
<section id="slide-org10d690d">
<h3 id="org10d690d">PCA for Feature Extraction</h3>

<div id="orga05d65a" class="figure">
<p><img src="./assets/pca-for-feature-extraction.png" alt="pca-for-feature-extraction.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>We can also "extract" features</li>
<li>Used to be very popular in images of faces</li>
<li>Though deep learning has kind of taken over this kind of thing</li>
<li>So we have a bunch of faces</li>
<li>And we want to detect who a particular face is</li>
<li>Naive: use the pixel space as inputs, i.e., the value of each pixel</li>
<li>That would be VERY high dimensional and probably wouldn't work very well</li>
<li>Instead we use PCA to find groups of pixels that are important together</li>
<li>And what you get are these new features, that kind of represent
various facial features</li>
<li>Some of these heads are tilted left or right, some downward facing, etc.</li>
<li>Some of it is also which direction does the light come from,
darkness of eyebrows, etc.</li>
<li>Each face picture is a linear combination of the PCA components,
weighted by the coefficients that a model learns</li>
<li>So is a person's face is facing left, you expect that component
coefficient to be high</li>
<li>So this is kind of cool, but again, this doesn't necessarily make
good classifications</li>
<li>So the first two seem to be able which side the light comes from,
which is an interesting feature of the picture but NOT of the
face&#x2026;</li>

</ul>

</aside>
</section>
<section id="slide-orgd8e7f84">
<h3 id="orgd8e7f84">1-NN and Eigenfaces</h3>
<font size=6>
<div class="org-src-container">

<pre   ><code class="python" >from sklearn.datasets import fetch_lfw_people
from sklearn.neighbors import KNeighborsClassifier
people = fetch_lfw_people(min_faces_per_person=35, resize=0.7)
X_people, y_people = people.data, people.target
</code></pre>
</div>

<div class="org-src-container">

<pre   ><code class="python" >X_train, X_test, y_train, y_test = \
    train_test_split(X_people, y_people,
                     stratify=y_people, random_state=0)
print(X_train.shape)
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)
print(f"{knn.score(X_test, y_test):.3f}")
</code></pre>
</div>

<pre class="example">
(1539, 5655)
0.311
</pre>


<div class="org-src-container">

<pre   ><code class="python" >pca = PCA(n_components=100, whiten=True, random_state=0)
pca.fit(X_train)
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
print(X_train_pca.shape)
</code></pre>
</div>

<pre class="example">

&gt;&gt;&gt; (1539, 100)
</pre>


<div class="org-src-container">

<pre   ><code class="python" >knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train_pca, y_train)
print(f"{knn.score(X_test_pca, y_test):.3f}")
</code></pre>
</div>

<pre class="example">
0.331
</pre>

</font>
<aside class="notes">
<ul>
<li>Something like 40 classes, very "wide"</li>
<li>The reason to use 1NN is it kinds of gives an idea of your
representation of the data</li>
<li>1NN just looks at the closest neighbor, so if it does well, then
your representation of the data is pretty good b/c you've basically
clustered your points together</li>
<li>Actually can only get as many components as min(n_features, n_samples)</li>
<li>So then here I pick 100 components (arbitrarily), whiten, and then do 1NN.</li>
<li>So we do better, although it's kind of weird that 1NN did okay
initially, normally you'd see something lower like 0.20.</li>
<li>It might be b/c I didn't balance the dataset, so the accuracy here
is not that meaningful. With balancing I'd expect to see 0.25 or so
for 1NN and basically the same for PCA.</li>

</ul>

</aside>
</section>
<section id="slide-org756a1f0">
<h3 id="org756a1f0">Reconstruction</h3>

<div id="org7b42067" class="figure">
<p><img src="./assets/reconstruction.png" alt="reconstruction.png" height="400px" />
</p>
</div>
<aside class="notes">
<ul>
<li>Another way to select how many components you want</li>
<li>Look at the reconstructions using X components</li>
<li>So here we're doing the same inverse_transform as before</li>
<li>But in this case it's intuitive to visualize each sample</li>
<li>Can also use PCA for detecting outliers&#x2026;</li>

</ul>

</aside>
</section>
</section>
<section>
<section id="slide-orgefa0ab8">
<h2 id="orgefa0ab8">PCA for Outlier Detection</h2>
<div class="outline-text-2" id="text-orgefa0ab8">
</div>
</section>
<section id="slide-org12d16a9">
<h3 id="org12d16a9">PCA for Outlier Detection</h3>
<div class="org-src-container">

<pre   ><code class="python" >pca = PCA(n_components=100).fit(X_train)
inv = pca.inverse_transform(pca.transform(X_test))
reconstruction_errors = np.sum((X_test - inv)**2, axis=1)
</code></pre>
</div>
<div class="column" style="float:left; width: 50%">

<div id="org0fbc5bf" class="figure">
<p><img src="./assets/best-reconstructions.png" alt="best-reconstructions.png" />
</p>
</div>

<p>
Best reconstructions
</p>
</div>
<div class="column" style="float:left; width: 50%">

<div id="org589747a" class="figure">
<p><img src="./assets/worst-reconstructions.png" alt="worst-reconstructions.png" />
</p>
</div>

<p>
Worst reconstructions
</p>
</div>
<aside class="notes">
<ul>
<li>Which faces are reconstructed well vs. not reconstructed well?</li>
<li>If not reconstructed well, they may be outliers!</li>
<li>Best reconstructions look like "most" of the data</li>
<li>Worst are kind of outliers: hats, lots of shirt collars, very
tilted, forehead cutoff, etc. Mostly about cropping of the images</li>
<li>Obviously applies to any kind of data</li>

</ul>

</aside>
</section>
</section>
<section>
<section id="slide-org1821040">
<h2 id="org1821040">Manifold Learning</h2>
<aside class="notes">
<ul>
<li>A class of algorithms that do much more complicated transformations
of the data.</li>
<li>A manifold is just a geometric structure in higher dimensions.</li>
<li>Idea is you have a smaller dimensional set embedded in a higher
dimensional space</li>

</ul>

</aside>
</section>
<section id="slide-org320c74e">
<h3 id="org320c74e">Manifold Learning</h3>

<div id="org0d455e2" class="figure">
<p><img src="./assets/manifold-learning-structure.png" alt="manifold-learning-structure.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>Learn underlying "manifold" structure, use for dimensionality reduction.</li>
<li>So here's an example. This is actually just a flat plane, but it's
been kind of rolled up in 3 dimensions.</li>
<li>PCA cannot figure this out b/c it's just linear</li>
<li>Ex: if it projects up/down, the purple will be right next to the red
points</li>
<li>Need nonlinear transformations</li>

</ul>

</aside>
</section>
<section id="slide-orgd788ef2">
<h3 id="orgd788ef2">Pros and Cons</h3>
<ul>
<li>For visualization only</li>
<li>Axes don’t correspond to anything in the input space.</li>
<li>Often can’t transform new data.</li>
<li>Pretty pictures!</li>

</ul>
<aside class="notes">
<ul>
<li>Mostly used just for visualizations</li>
<li>Hard to interpret</li>
<li>can't really figure out how to map new data samples onto the
manifold, although some of them have been extended to be able to do
this</li>
<li>Pretty good for exploring data</li>

</ul>

</aside>
</section>
<section id="slide-orgf8164e3">
<h3 id="orgf8164e3">Algorithms in sklearn</h3>
<ul>
<li><code>KernelPCA</code> – does PCA, but with kernels 
<ul>
<li>Eigenvalues of kernel-matrix</li>

</ul></li>
<li>Spectral embedding (Laplacian Eigenmaps) 
<ul>
<li>Uses eigenvalues of graph Laplacian</li>

</ul></li>
<li>Locally Linear Embedding</li>
<li>Isomap “kernel PCA on manifold”</li>
<li>t-SNE (t-distributed stochastic neighbor embedding)</li>

</ul>
<aside class="notes">
<ul>
<li>Kernel: don't worry too much about the idea, but mathematically it's
similar to how we used kernel SVMs to simulate adding polynomial
features</li>
<li>A lot of these are nice and have their uses&#x2026;</li>
<li>&#x2026;But we're only going to talk about t-SNE</li>
<li>The least strong theory about t-SNE, but used a lot in practice</li>

</ul>

</aside>
</section>
</section>
<section>
<section id="slide-org2bd0070">
<h2 id="org2bd0070">t-SNE</h2>
<p>
\[p_{j\mid i} = \frac{\exp(-\lVert\mathbf{x}_i - \mathbf{x}_j\rVert^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\lVert\mathbf{x}_i - \mathbf{x}_k\rVert^2 / 2\sigma_i^2)}\]\[p_{ij} = \frac{p_{j\mid i} + p_{i\mid j}}{2N}\]
</p>
<p>
\[q_{ij} = \frac{(1 + \lVert \mathbf{y}_i - \mathbf{y}_j\rVert^2)^{-1}}{\sum_{k \neq i} (1 + \lVert \mathbf{y}_i - \mathbf{y}_k\rVert^2)^{-1}}\]
</p>
<p>
\[KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}\]
</p>
<aside class="notes">
<ul>
<li>Main idea: find a probability distribution over your data points</li>
<li>given one data point, if I pick one neighbors at random but weighted
by distance, how likely to pick neighbor j</li>
<li>Intuitively, for each data point, look at the density of the neighbors</li>
<li>Make symmetric</li>
<li>It's a distribution over pairs of points</li>
<li>Then assume we have some embedding, those are the y points (x are original)</li>
<li>Look at a similar thing but distance between the ys</li>
<li>What we want is for these two distributions to be the same</li>
<li>So we try to minimize the bottom thing</li>
<li>&#x2014;</li>
<li>Starts with a random embedding</li>
<li>Iteratively updates points to make "close" points close. (gradient descent)</li>
<li>Global distances are less important, neighborhood counts.</li>
<li>Good for getting coarse view of topology.</li>
<li>Can be good for finding interesting data point</li>
<li>t distribution heavy-tailed so no overcrowding.</li>
<li>(low perplexity: only close neighbors &#x2013; hyperparameter)</li>

</ul>

</aside>
</section>
<section id="slide-org1fa268b">
<h3 id="org1fa268b"></h3>
<div class="org-src-container">

<pre   ><code class="python" >from sklearn.manifold import TSNE
from sklearn.datasets import load_digits
digits = load_digits()
X = digits.data / 16.
X_tsne = TSNE().fit_transform(X)
X_pca = PCA(n_components=2).fit_transform(X)
</code></pre>
</div>
<div class="column" style="float:left; width: 50%">

<div id="org07d9eb2" class="figure">
<p><img src="./assets/pca-digits.png" alt="pca-digits.png" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">

<div id="orga5fd5a0" class="figure">
<p><img src="./assets/tsne-digits.png" alt="tsne-digits.png" />
</p>
</div>
</div>
<aside class="notes">
<ul>
<li>First PCA embedding, which is kind of a mess</li>
<li>Then t-SNE embedding, which separates much nicer</li>
<li>Can help understand the data</li>

</ul>

</aside>
</section>
<section id="slide-orgba40440">
<h3 id="orgba40440"></h3>

<div id="org3ae8e0d" class="figure">
<p><img src="./assets/tsne-embeddings-digits.png" alt="tsne-embeddings-digits.png" height="400px" />
</p>
</div>
<aside class="notes">
<ul>
<li>Different transformation (it's a random algorithm)</li>
<li>This time with some of the digit images shown</li>
<li>Notice that some digits get grouped into multiple clusters based on
how they're written</li>

</ul>

</aside>
</section>
<section id="slide-org36af977">
<h3 id="org36af977">Tuning t-SNE perplexity</h3>
<div class="column" style="float:left; width: 50%">

<div id="orgd878255" class="figure">
<p><img src="./assets/tsne-tuning-2.png" alt="tsne-tuning-2.png" height="250px" />
</p>
</div>


<div id="org180765b" class="figure">
<p><img src="./assets/tsne-tuning-5.png" alt="tsne-tuning-5.png" height="250px" />
</p>
</div>
</div>
<div class="column" style="float:right; width: 50%">

<div id="org53bf06c" class="figure">
<p><img src="./assets/tsne-tuning-30.png" alt="tsne-tuning-30.png" height="250px" />
</p>
</div>


<div id="orge0f52e5" class="figure">
<p><img src="./assets/tsne-tuning-300.png" alt="tsne-tuning-300.png" height="250px" />
</p>
</div>
</div>
<aside class="notes">
<ul>
<li>Important parameter: perplexity</li>
<li>Intuitively: bandwidth of neighbors to consider</li>
<li>(low perplexity: only close neighbors)</li>
<li>smaller datasets try lower perplexity</li>
<li>original authors say 30 always works well.</li>
<li>but depends on the dataset somewhat</li>

</ul>

</aside>
</section>
<section id="slide-org3f05ef4">
<h3 id="org3f05ef4"></h3>
<div class="column" style="float:left; width: 50%">

<div id="org8e94158" class="figure">
<p><img src="./assets/tsne-moons.png" alt="tsne-moons.png" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">

<div id="orgd0e7ef6" class="figure">
<p><img src="./assets/tsne-perplexity.png" alt="tsne-perplexity.png" />
</p>
</div>
</div>
<aside class="notes">
<ul>
<li>Same for synthetic two moons dataset</li>
<li>Again perplexity=30 seems to work well</li>

</ul>

</aside>
</section>
<section id="slide-org28b4b9a">
<h3 id="org28b4b9a">Play around online</h3>
<p>
<a href="http://distill.pub/2016/misread-tsne/">http://distill.pub/2016/misread-tsne/</a>
</p>
<aside class="notes">
<ul>
<li>Interactive javascript visualizations</li>
<li>Really cool to see how the outcome of t-SNE changes on different
datasets</li>
<li>BTW, t-SNE is much slower than PCA</li>
<li>Also you can't really use this as to transform the test set, there
are weird hacky things to do but it's not in sklearn yet and not
straightforward</li>

</ul>

</aside>
</section>
</section>
<section>
<section id="slide-orgc72963e">
<h2 id="orgc72963e">Discriminant Analysis</h2>
<aside class="notes">
<ul>
<li>In particular <b>linear</b> DA</li>
<li>It's a classifier, and can also be used for dimensionality reduction</li>

</ul>

</aside>
</section>
<section id="slide-org653e6cd">
<h3 id="org653e6cd">Linear Discriminant Analysis aka Fisher Discriminant</h3>
<font size=6>
<p>
\[    P(y=k | X) = \frac{P(X | y=k) P(y=k)}{P(X)} = \frac{P(X | y=k) P(y = k)}{ \sum_{l} P(X | y=l) \cdot P(y=l)}\]
</p>
<p>
\[ p(X | y=k) = \frac{1}{(2\pi)^n |\Sigma|^{1/2}}\exp\left(-\frac{1}{2} (X-\mu_k)^t \Sigma^{-1} (X-\mu_k)\right) \]
</p>
<p>
\[    \log\left(\frac{P(y=k|X)}{P(y=l | X)}\right) = 0 \Leftrightarrow (\mu_k-\mu_l)\Sigma^{-1} X = \frac{1}{2} (\mu_k^t \Sigma^{-1} \mu_k - \mu_l^t \Sigma^{-1} \mu_l) \]
</p>
</font>
<aside class="notes">
<ul>
<li>Generative model: assumes each class has Gaussian distribution</li>
<li>First eq is Bayes rule, then expand over all classes</li>
<li>Basically we're fitting a Gaussian to each class, but we're sharing
the covariance matrix</li>
<li>For each class, estimate mean, then compute covariance matrix</li>
<li>Very fast: only compute means and invert covariance matrix (works
well if n_features &lt;&lt; n_samples)</li>
<li>Last line is just writing it in an equivalent way</li>
<li>Leads to linear decision boundary.</li>
<li>Imagine: transform space by covariance matrix (rotate and stretch),
then nearest centroid</li>
<li>No parameters to tune!</li>
<li>Don't confuse with Latent Dirichlet Allocation (LDA)</li>

</ul>

</aside>
</section>
<section id="slide-org68e7ffd">
<h3 id="org68e7ffd">Quadratic Discriminant Analysis</h3>
<p>
\[ p(X | y=k) = \frac{1}{(2\pi)^n |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (X-\mu_k)^t \Sigma_k^{-1} (X-\mu_k)\right) \]
</p>
<aside class="notes">
<ul>
<li>Each class is Gaussian, but separate covariance matrices!</li>
<li>More flexible (quadratic decision boundary), but less robust: have less points per covariance matrix.</li>
<li>Can't think of it as transformation of the space.</li>
<li>Many more parameters; harder to fit</li>

</ul>

</aside>
</section>
<section id="slide-org6e58b0f">
<h3 id="org6e58b0f">Comparison</h3>

<div id="org077539a" class="figure">
<p><img src="./assets/linear-vs-quadratic-discriminant-analysis.png" alt="linear-vs-quadratic-discriminant-analysis.png" height="400px" />
</p>
</div>
<aside class="notes">
<ul>
<li>If the two features have same covariance matrices, LDA does fine</li>
<li>Otherwise QDA does better</li>

</ul>

</aside>
</section>
<section id="slide-orga26d66d">
<h3 id="orga26d66d">Discriminants and PCA</h3>
<ul>
<li>Both fit Gaussian model</li>
<li>PCA for the whole data</li>
<li>LDA multiple Gaussians with shared covariance</li>
<li>Can use LDA to transform space</li>
<li>At most as many components as there are classes - 1 (needs
between-class variance)</li>

</ul>
<aside class="notes">
<ul>
<li>Can use it as a linear classifier (but probably use logreg instead)</li>

</ul>

</aside>
</section>
<section id="slide-orgcd760aa">
<h3 id="orgcd760aa">PCA vs Linear Discriminants</h3>

<div id="orgcbe590d" class="figure">
<p><img src="./assets/pca-lda.png" alt="pca-lda.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>Example with PCA vs. LDA</li>
<li>Three classes, synthetic datasets</li>
<li>LDA needs only one dimension, the interesting one</li>

</ul>

</aside>
</section>
<section id="slide-org023d87c">
<h3 id="org023d87c">Data where PCA failed</h3>

<div id="org2cb9a06" class="figure">
<p><img src="./assets/pca-fail.png" alt="pca-fail.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>Remember where PCA failed</li>
<li>In this case LDA immediately finds the best component</li>
<li>Note this is actually supervised, in case you didn't catch it before</li>

</ul>

</aside>
</section>
</section>
<section>
<section id="slide-org7e44baf">
<h2 id="org7e44baf">Summary</h2>
<ul>
<li>PCA good for visualization, exploring correlations</li>
<li>PCA can sometimes help with classification as regularization or for
feature extraction</li>
<li>Manifold learning makes nice pictures</li>
<li>LDA is a supervised alternative to PCA</li>

</ul>
<aside class="notes">
<p>
LDA also yields a rotation of the input space
</p>

</aside>
</section>
</section>
</div>
</div>
<script src="./revealjs/dist/reveal.js"></script>
<script src="./revealjs/plugin/highlight/highlight.js"></script>
<script src="./revealjs/plugin/notes/notes.js"></script>


<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
plugins: [RevealHighlight, RevealNotes],
slideNumber:true, hash:true
});

</script>
</body>
</html>
